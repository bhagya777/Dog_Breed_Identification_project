# -*- coding: utf-8 -*-
"""Dog_Breed_Idendification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1j6sVvgWaH_Msd3qyT1MNDkqnjunnmp
"""

import pandas as pd
import numpy as np

#!unzip "drive/MyDrive/Dog_Breed_Identification_project/dog-breed-identification.zip" -d "drive/MyDrive/Dog_Breed_Identification_project/"

"""# Dog Breed Identification Project
  TensorFlow is used

## 1. Problem Statement:
      Identify Dog breed of the given dog's picture.

## 2. Data:
      From Kaggle
      Unstructured Data(Images)

## 3. Evaluation:
      A file with prediction probabilities for each dog breed of each test image.

## 4. Features:
      * There are 120 breeds of dog.
      * 10000+ images in training set. (with labels)
      * 10000+ images in test set. (no labels)

# Getting workspace Ready

  * Import TensorFlow 2.x
  * Import TensorFlow Hub
  * Use GPU
"""

import tensorflow as tf
import tensorflow_hub as hub
print("TensorFlow version: ",tf.__version__)
print("TensorFlow Hub version:",hub.__version__)

# Check GPU availability
print("GPU", "available!!!" if tf.config.list_physical_devices("GPU") else "not available!!!")

"""# Getting Data Ready (Turning It into Tensors)
   Turning Data (Images) into Tensors i.e. Numerical Representation.
"""

import pandas as pd
labels_csv=pd.read_csv("drive/MyDrive/Dog_Breed_Identification_project/labels.csv")
print(labels_csv.describe())
labels_csv.head()

labels_csv["breed"].value_counts()

labels_csv["breed"].value_counts().plot.bar(figsize=(20,10))

labels_csv["breed"].value_counts().median()

from IPython.display import Image
Image("drive/MyDrive/Dog_Breed_Identification_project/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")

"""# Getting Images and Labels
  Get list of Image File pathnames.
"""

filenames=["drive/MyDrive/Dog_Breed_Identification_project/train/"+ filename +".jpg" for filename in labels_csv["id"]]
filenames[:10]

#Checking if number of filenames match number of images
import os
if len(os.listdir("drive/MyDrive/Dog_Breed_Identification_project/train/"))==len(filenames):
  print("Filenames match.Proceed!")
else:
  print("Filenames number not match.")

Image(filenames[9000])

labels_csv["breed"][9000]

"""###  Converting Labels into numbers"""

import numpy as np
labels=labels_csv["breed"]
labels=np.array(labels)
labels

len(labels)

"""### Checking Missing Data"""

if len(labels)==len(filenames):
  print("No missing Data!")
else:
  print("Some Data is Missing!")

"""#### Finding unique labels ie. breeds."""

unique_breeds=np.unique(labels)
len(unique_breeds)

"""#### Turning every label into boolean array"""

boolean_labels=[label==unique_breeds for label in labels]
boolean_labels[:2]

len(boolean_labels)

print(labels[0])
print(np.where(unique_breeds==labels[0]))
print(boolean_labels[0].argmax())
print(boolean_labels[0].astype(int))

"""# Splitting Data into Training and Validation Data set"""

x=filenames
y=boolean_labels

NUM_IMAGES=1000 #@param{type:"slider",min:1000,max:10000,step:1000}

from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val=train_test_split(x[:NUM_IMAGES],
                                             y[:NUM_IMAGES],
                                             test_size=0.2,
                                             random_state=1)
len(x_train),len(x_val),len(y_train),len(y_val)

x_train[:5],y_train[:2]

"""# Preprocessing (Turning Images into Tensors)

Function should:
  1. Take image filepath as input.
  2. Use TensorFlow to read the file and save it to a variable, `image`.
  3. Turn `image` (.jpg) into tensors.
  4. Normalize `image` Convert colour channel values fro 0-255 to 0 to 1.
  5. Resize `image` to be in shape of (250,250).
  6. Return the modified `image`.
"""

from matplotlib.pyplot import imread
image=imread(filenames[9000])
image.shape

image[:2]

image.max(), image.min()

tf.constant(image)[:2]

"""## Function Creation"""

IMG_SIZE=224

def process_image(image_path,img_size=IMG_SIZE):
  """
  Takes file patrh as input and gives tensor as output
  """
  image=tf.io.read_file(image_path)
  #Converting image to numerical tensor with 3 colour channels ie. Red, Green, Blue
  image=tf.image.decode_jpeg(image,channels=3)
  #Converting colour channel values from 0-255 to 0 to 1.
  image=tf.image.convert_image_dtype(image, tf.float32)
  #Resize the image
  image=tf.image.resize(image,size=[IMG_SIZE,IMG_SIZE])

  return image

"""## Turning Data into Batches
   As trying to process all the 10000+ images at once will be difficult (will be slow) as they won't fit in GPU's memory.
   Let's make batch size 20 ie. 20 images at a time.

   In order to use TensorFlow effectively, data should be in the form of tensor couples like:
   `(image,label)`

### Function for returning tuple (image, label)
"""

def get_image_label(image_path,label):
  """
  Takes file path and label, processes image and returns tuple of (image, label)
  """
  image=process_image(image_path)
  return image,label

(process_image(x[32]),tf.constant(y[32]))

BATCH_SIZE=20

def create_data_batches(x,y=None, batch_size=BATCH_SIZE,valid_data=False,test_data=False):
  """
  Convert Data (x abd y pairs) into batches.
  Shuffles data if it's training data but doesn't shuffle validation data.
  Also accepts test data as input (no labels).
  """

  # If Test Data (No Labels)
  if test_data:
    print("Creating Test Data Batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x)))   #Only filepaths No Labels
    data_batch=data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  # If Valid Data (No shuffling required)
  elif valid_data:
    print("Creating Valid Data Batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))      #Filepaths,Labels
    data_batch=data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating Training Data Batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))
    # Shuffling before Mapping is faster
    data=data.shuffle(buffer_size=len(x))
    data=data.map(get_image_label)
    data_batch=data.batch(BATCH_SIZE)
  return data_batch

train_data=create_data_batches(x_train,y_train)
val_data=create_data_batches(x_val,y_val,valid_data=True)

#Checking Different attributes of Data batches
train_data.element_spec, val_data.element_spec

"""###Visualizing Data Batches"""

import matplotlib.pyplot as plt

def show_20_images(images,labels):
  plt.figure(figsize=(10,10))
  for i in range(20):
    # 4 rows 4 columns i+1 index
    ax=plt.subplot(5,4,i+1)
    plt.imshow(images[i])
    #gives label name where label has highest value(ie. True or 1) in labels list in unique breeds.
    plt.title(unique_breeds[labels[i].argmax()])
    # To turn gridlines off
    plt.axis("off")

train_data

"""Turn `train_data` `val_data` into an iterator type. (Right now it is Batch Dataset type.)"""

#Everytime this cell is run images will be new as data is shuffled
train_images,train_labels=next(train_data.as_numpy_iterator())
train_images,train_labels

len(train_images),len(train_labels)

show_20_images(train_images,train_labels)

val_images, val_labels=next(val_data.as_numpy_iterator())
show_20_images(val_images,val_labels)

"""# Building a Model
   Things to define:
   1. The input shape (image shape in the form of tensor) to the model.
   2. The output shape (labels shape in the form of tensor) of the model.
   3. The URL of the Model we want to use.

URL is from TensorFlow Hub : "https://kaggle.com/models/google/mobilenet-v2/TensorFlow2/130-224-classification/1"

We are goint to use Sequential Keras API
"""

INPUT_SHAPE=[None,IMG_SIZE,IMG_SIZE,3]       #batch, height, width, color channels
OUTPUT_SHAPE=len(unique_breeds)

MODEL_URL="https://kaggle.com/models/google/mobilenet-v2/TensorFlow2/130-224-classification/1"

"""### Puttng input, output, model into Keras Deep Learning Model.
  * The Function Should :
    1. INPUT_SHAPE, OUTPUT_SHAPE, Model as parameters.
    2. Define layers in Keras MOdel in Sequential Model.
    3. Compiles Model (It should be evaluated and improved).
    4. Builds the Model (Tells Model input shape it will be getting).
    5. Returns the Model.

Refer: https://www.tensorflow.org/guide/keras

"""

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building Model with:", MODEL_URL)

  #Setting up the layers
  model=tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=input_shape[1:]),  #Layer 1 (Input layer)
    tf.keras.layers.Lambda(lambda x: hub.KerasLayer(MODEL_URL)(x)),
    tf.keras.layers.Dense(units=OUTPUT_SHAPE,activation="softmax")   #Layer 2 (Output Layer)
])

  #Compile The Model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )

  #Build The Model
  model.build(INPUT_SHAPE)

  return model

"""##### `softmax` is used for Multi class classification similar to `sigmoid` is used for Binary classification."""

model=create_model()
model.summary()

"""###  Creating callbacks
Callbacks are helper functions a model can use. It checks model's progress, saves the progress, stop training it early if the model stops improving.

We are going to create two callbacks here, one for TensorBoard (used to understand model's progress and improve model's performance by updating hyperparameters) to track model's progress and another for early stopping thereby preventing it from training too long.

#### TensorFlow Callback
Steps:                                               
1. Load TensorBoard notebook extension
2. Create TensorFlow Callback to save logs to a directory and pass it to a model's `fit()` function
3. Visualize model's training logs `%tensorboard` magic function
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import datetime

def create_tensorboard_callback():
  #Creating log directory for storing tensorboard logs
  logdir=os.path.join("drive/MyDrive/Dog_Breed_Identification_project/logs",
                      datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

"""#### Early Stopping Callback
It helps prevent model from overfitting by stopping training if a certain evaluation metric stops improving.

Refer: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping
"""

early_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",patience=3)

"""# Training the Model
Model is trained on subset of 1000 images
"""

NUM_EPOCHS=100 #@param{type:"slider",min:10,max:100,step:10}

"""## Creating Fuction
Function should:
1. Create a model using `create_model()`
2. Set up Tensorboard callback using `create_tensorboard_callback()`
3. Call `fit()` function. Pass it training data, validation data, number of epochs to train for (NUM_EPOCHS) and the callback
4. Return the model
"""

def train_model():
  model=create_model()
  tensorboard=create_tensorboard_callback()
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard,early_stopping])
  return model

"""Epochs means number of times model should be trained on training set. validation_frq means number of times model should be tested on validation set. First epoch requires longest time compared to the remaining."""

model=train_model()

"""## Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Dog_Breed_Identification_project/logs

"""# Prediction and Evaluation Using Trained Model

## Making Predictions
"""

predictions=model.predict(val_data, verbose=1)
predictions

predictions.shape

len(y_val)

predictions[0]

np.sum(predictions[0])

"""## Unbatch the batched dataset (validation) and compare the predicted labels with actual ones (truth)."""

def get_the_prediction(prediction_probabilities):
  pred_label=unique_breeds[np.argmax(prediction_probabilities)]
  return pred_label
get_the_prediction(predictions[0])

def unbatchify(data):
  images=[]
  labels=[]
  for image,label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images, labels

val_images, val_labels=unbatchify(val_data)
val_images[0], val_labels[0]

get_the_prediction(val_labels[0])

"""## Plot the image with labels (predicted and actual with confidence"""

def plot_pred(prediction_probabilities,labels,images,n=1):
  pred_prob,true_label,image=prediction_probabilities[n],labels[n],images[n]

  pred_label=get_the_prediction(pred_prob)

  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  if pred_label==true_label:
    color="green"
  else:
    color="red"

  plt.title("{}{:2.0f}%{}".format(pred_label,np.max(pred_prob)*100,true_label),color=color)

plot_pred(prediction_probabilities=predictions,labels=val_labels,images=val_images,n=0)

"""## Plot top 10 images according to confidences"""

def top_10_conf(prediction_probabilities,labels,n=1):
  pred_prob,true_label=prediction_probabilities[n],labels[n]

  pred_label=get_the_prediction(pred_prob)

  top_10_pred_indexes=pred_prob.argsort()[-10:][::-1]
  top_10_confidence_values=pred_prob[top_10_pred_indexes]
  top_10_pred_labels=unique_breeds[top_10_pred_indexes]

  top_plot=plt.bar(np.arange(len(top_10_pred_labels)),top_10_confidence_values,color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),labels=top_10_pred_labels,rotation="vertical")

  if np.isin(true_label,top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels==true_label)].set_color("green")
  else:
    pass

top_10_conf(predictions,val_labels,1)

i_multiplier=0
num_rows=3
num_cols=2
num_images=num_rows*num_cols
plt.figure(figsize=(10*num_cols,5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows,2*num_cols,2*i+1)
  plot_pred(prediction_probabilities=predictions,labels=val_labels,images=val_images,n=i+i_multiplier)
  plt.subplot(num_rows,2*num_cols,2*i+2)
  top_10_conf(prediction_probabilities=predictions,labels=val_labels,n=i+i_multiplier)

plt.tight_layout(h_pad=1.0)
plt.show()

"""# Saving and Reloading Trained Model"""

def save_model(model,suffix=None):
  modeldir=os.path.join("drive/MyDrive/Dog_Breed_Identification_project/models",datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path=modeldir+"-"+suffix+".h5"
  print(f"Saving Model to: {model_path}...")
  model.save(model_path)
  print(model_path)

def load_model(model_path):
  print(f"Loading model from: {model_path}...")
  model=create_model()
  model.load_weights(model_path)
  return model

save_model(model,suffix="1000-images-MobileNetV2-Adam")

loaded_model_1000_images=load_model("drive/MyDrive/Dog_Breed_Identification_project/models/20241225-08011735113692-1000-images-MobileNetV2-Adam.h5")

model.evaluate(val_data)

loaded_model_1000_images.evaluate(val_data)

"""# Training the model on full Dataset"""

full_data=create_data_batches(x,y)

full_data

full_model=create_model()

full_model_tensorboard=create_tensorboard_callback()
full_model_early_stopping=tf.keras.callbacks.EarlyStopping(monitor="accuracy",patience=3)

full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard,full_model_early_stopping])

save_model(full_model,suffix="full-image-set-mobilenetv2-Adam")

"""# Making Predictions"""

loaded_full_model=load_model("drive/MyDrive/Dog_Breed_Identification_project/models/20241225-08221735114970-full-image-set-mobilenetv2-Adam.h5")

test_path="drive/MyDrive/Dog_Breed_Identification_project/test/"
test_filenames=[test_path+fname for fname in os.listdir(test_path)]
test_filenames[:10]

len(test_filenames)

test_data=create_data_batches(test_filenames,test_data=True)

test_data

test_predictions=loaded_full_model.predict(test_data,verbose=1)

np.savetxt("drive/MyDrive/Dog_Breed_Identification_project/predictions.csv",test_predictions,delimiter=",")

test_predictions=np.loadtxt("drive/MyDrive/Dog_Breed_Identification_project/predictions.csv",delimiter=",")

test_predictions[:10]

test_predictions.shape

"""# Submission to Kaggle
Making the Dataframe to export as csv to subit in requred format in Kaggle
"""

preds_df=pd.DataFrame(columns=["id"]+list(unique_breeds))
preds_df.head()

test_ids=[os.path.splitext(path)[0] for path in os.listdir(test_path)]
test_ids

os.path.splitext(test_filenames[0])

preds_df["id"]=test_ids
preds_df.head()

preds_df[list(unique_breeds)]=test_predictions
preds_df.head()

preds_df.to_csv("drive/MyDrive/Dog_Breed_Identification_project/Full_Model_Predictions_Submission_mobilenetv2.csv",index=False)

